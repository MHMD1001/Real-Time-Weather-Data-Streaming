{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d55552-c17e-4d2a-b622-cf6c8cbe0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ad225-3e33-47e5-b28a-1eb9b69f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession \n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"test_streaming\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a721c73-14ba-497f-8f0e-1e2eb9077c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka\n",
    "kafka_df = spark.readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\",\"broker:29092\")\\\n",
    "        .option(\"subscribe\",\"weather_data\")\\\n",
    "        .option(\"startingOffsets\",\"earliest\")\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987ffed-b5d6-4d25-9c28-bd52bfbf7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to parse streaming data\n",
    "def parsing_streaming_data(data):\n",
    "    # Defining a schema\n",
    "    schema = StructType([\n",
    "    StructField(\"city_id\", StringType()),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"weather_condition\", StringType(), True),\n",
    "    StructField(\"weather_description\", StringType(), True),\n",
    "    StructField(\"temperature\", DecimalType(5,2), True),\n",
    "    StructField(\"min_temp\", DecimalType(5,2), True),\n",
    "    StructField(\"max_temp\", DecimalType(5,2), True),\n",
    "    StructField(\"pressure\", IntegerType(), True),\n",
    "    StructField(\"humidity\", DecimalType(4,2), True),\n",
    "    StructField(\"wind_speed\", DecimalType(4,2), True),\n",
    "    StructField(\"visibility\", DecimalType(6,1), True),\n",
    "    StructField(\"creation_time\", TimestampType())\n",
    "])\n",
    "    # Convert the value column to json string and store it in a new dataframe\n",
    "    stream_data =data.selectExpr(\"Cast (value As string)\")\n",
    "    # Parse json and apply schema\n",
    "    parsed_data = stream_data.select(fn.from_json(fn.col(\"value\"), schema).alias(\"data\"))\n",
    "    # Return the parsed data\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98d454-347b-42fb-a577-2777e36905c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to select the relevant fields from the streamed data\n",
    "def select_columns(data): \n",
    "    # Create a dataframe with the relevant column\n",
    "    selected_data = data.select('data.*')\n",
    "    # Return a df with the relevant fields\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adddf37-7c51-44b5-9664-603a986d6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define to make some transformations\n",
    "def transform_measurement_units(data):\n",
    "    # Create a temp view from the data to do trnsformations with spark sql\n",
    "    data.createOrReplaceTempView(\"streaming_data\")\n",
    "    # Making the necessary Transformations\n",
    "    transformed_data = spark.sql(\"\"\"SELECT \n",
    "                                        city_id,\n",
    "                                        city,\n",
    "                                        weather_condition,\n",
    "                                        weather_description,\n",
    "                                        Round(temperature-272.15,2) AS temperature,\n",
    "                                        Round(min_temp-272.15,2) AS min_temp,\n",
    "                                        Round(max_temp-272.15,2) AS max_temp,\n",
    "                                        pressure,\n",
    "                                        humidity,\n",
    "                                        ROUND(wind_speed*3.6,2) AS wind_speed,\n",
    "                                        visibility,\n",
    "                                        creation_time,\n",
    "                                        TO_DATE (creation_time,\"yyy-MM-dd\") AS creation_date\n",
    "                                From streaming_data\"\"\")\n",
    "    #Return the transformed df\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867a137-f564-432b-8f5a-b94ef81b604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the streamed data\n",
    "parsed_df = parsing_streaming_data(kafka_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb051a-8130-40d7-bb28-8d844a72113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the relevant columns from the streamed data\n",
    "weather_df = select_columns(parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b108ca-5ee9-4d6a-b7f1-9b842468d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make transformations to the data\n",
    "transformed_df = transform_measurement_units(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979f357-386d-426a-9219-cf0abd96c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to start HDFS streaming query\n",
    "def start_hdfs_streaming_query (data):\n",
    "    query = data \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://namenode:9000/data/streaming/checkpoints/checkpoint1\") \\\n",
    "    .option(\"path\", \"hdfs://namenode:9000/data/streaming/weather_data/weather_data.parquet\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9fab2-e81e-4bce-95ca-c71caf876e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start HDFS streaming query\n",
    "start_hdfs_streaming_query(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7eec2e-d686-4b6b-8206-6154bc549dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create Hive table using spark sql\n",
    "def creating_hive_taple(data):    \n",
    "    hive_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS default.weather_data (\n",
    "        city_id STRING,\n",
    "        city STRING,\n",
    "        weather_condition STRING,\n",
    "        weather_description STRING,\n",
    "        temperature DECIMAL(5,2),\n",
    "        min_temp DECIMAL(5,2),\n",
    "        max_temp DECIMAL(5,2),\n",
    "        pressure INTEGER,\n",
    "        humidity DECIMAL(4,2),\n",
    "        wind_speed DECIMAL(4,2),\n",
    "        visibility DECIMAL(6,1),\n",
    "        creation_time TIMESTAMP,\n",
    "        creation_date DATE\n",
    "    )\n",
    "    PARTITIONED BY (creation_date)\n",
    "    LOCATION 'hdfs://namenode:9000/data/streaming/weather_data/hive_table'\n",
    "    \"\"\"    \n",
    "    # Creating hive table\n",
    "    spark.sql(hive_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df754a7-2bce-441d-a698-a021e2e64b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to start Hive streaming query\n",
    "def start_hive_streaming_query(data):\n",
    "    query = data \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(lambda batch_df, batch_id: batch_df.write.mode(\"append\").insertInto(\"weather_data\")) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba4634-a193-483f-bd27-df1b2a825820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive table\n",
    "creating_hive_taple(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8373625-eef1-4216-be2d-368e17670c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Hive streaming query\n",
    "query = start_hive_streaming_query(transformed_df)\n",
    "query.awaitTermination() # Wait for query termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72265e9-b4df-4935-b442-cf76f6bd22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a4b76-a654-44e2-ba4d-b04615158e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685bbfe-998c-4e31-a235-5d8d674d78e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb21f88-7ab8-41d0-b356-65f70d9cf7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79315a7b-7504-470c-af89-8bd83dab2a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d9753-e852-401f-937e-ce01a6e9c624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a4a33-7d91-4aac-a1f1-ab004f6e606e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dfc1f-f6f6-46ac-ac21-0d6ec985c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2cec7-01ab-4c87-975c-5bdd6f25358d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f142fae-2ba7-4dbe-ba66-b3c0cffebad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0067a-1e84-4497-8e0a-e24ee3d31a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614f181-1fb9-45f3-aa01-faa9671acb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dbe22-e173-4a8f-a79b-0133ef808f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145694f2-45c5-4177-9772-0acf4142db6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559474c-30b0-4c41-90dd-b7fe95ac482e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c2ec7-9983-4936-9189-cefccfffb19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
